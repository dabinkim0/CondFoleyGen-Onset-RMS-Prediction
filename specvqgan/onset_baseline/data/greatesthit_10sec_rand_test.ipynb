{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import csv\n",
    "import glob\n",
    "import h5py\n",
    "import io\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import random\n",
    "import scipy\n",
    "import soundfile as sf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import cv2\n",
    "from scipy.signal import firwin, lfilter, ellip, filtfilt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from config import _C as config\n",
    "# import kornia as K\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data import *\n",
    "from utils import sound, sourcesep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_phased_filter(x:np.ndarray):\n",
    "    '''Zero-phased low-pass filtering'''\n",
    "    b, a = ellip(4, 0.01, 120, 0.125) \n",
    "    x = filtfilt(b, a, x, method=\"gust\")\n",
    "    return x\n",
    "\n",
    "def mu_law(rms:torch.Tensor, mu:int=255):\n",
    "    '''Mu-law companding transformation'''\n",
    "    # assert if all values of rms are non-negative\n",
    "    assert torch.all(rms >= 0), f'All values of rms must be non-negative: {rms}'\n",
    "    mu = torch.tensor(mu)\n",
    "    mu_rms = torch.sign(rms) * torch.log(1 + mu * torch.abs(rms)) / torch.log(1 + mu)\n",
    "    return mu_rms\n",
    "\n",
    "def inverse_mu_law(mu_rms:torch.Tensor, mu:int=255):\n",
    "    '''Inverse mu-law companding transformation'''\n",
    "    assert torch.all(mu_rms >= 0), f'All values of rms must be non-negative: {mu_rms}'\n",
    "    mu = torch.tensor(mu)\n",
    "    rms = torch.sign(mu_rms) * (torch.exp(mu_rms * torch.log(1 + mu)) - 1) / mu\n",
    "    return rms\n",
    "\n",
    "@torch.no_grad\n",
    "def get_mu_bins(mu, num_bins, rms_min):\n",
    "    mu_bins = torch.linspace(mu_law(torch.tensor(rms_min)), 1, steps=num_bins)\n",
    "    mu_bins = inverse_mu_law(mu_bins, mu)\n",
    "    return mu_bins\n",
    "\n",
    "def discretize_rms(rms, mu_bins):\n",
    "    rms = torch.maximum(rms, torch.tensor(0.0)) # change negative values to zero\n",
    "    rms_inds = torch.bucketize(rms, mu_bins, right=True) # discretize\n",
    "    return rms_inds\n",
    "\n",
    "def undiscretize_rms(rms_inds, mu_bins, ignore_min=True):\n",
    "    if ignore_min and mu_bins[0] > 0.0:\n",
    "        mu_bins[0] = 0.0\n",
    "    \n",
    "    rms_inds_is_cuda = rms_inds.is_cuda\n",
    "    if rms_inds_is_cuda:\n",
    "        device = rms_inds.device\n",
    "        rms_inds = rms_inds.detach().cpu()\n",
    "    rms = mu_bins[rms_inds]\n",
    "    if rms_inds_is_cuda:\n",
    "        rms = rms.to(device)\n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    loads image, flow feature, audio files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, list_file, frame_dir, audio_dir, config, split='train', max_sample=-1):\n",
    "        self.split = split\n",
    "        self.frame_rate = config.frame_rate\n",
    "        self.duration = config.duration\n",
    "        self.video_samples = config.video_samples\n",
    "        self.audio_samples = config.audio_samples\n",
    "        # self.mel_samples = config.mel_samples\n",
    "        self.audio_len = config.audio_samples # seconds\n",
    "        self.audio_sample_rate = config.audio_sample_rate\n",
    "        self.rms_samples = config.rms_samples\n",
    "        self.rms_nframes = config.rms_nframes\n",
    "        self.rms_hop = config.rms_hop\n",
    "        self.rms_discretize = config.rms_discretize\n",
    "        if self.rms_discretize:\n",
    "            self.rms_mu = config.rms_mu\n",
    "            self.rms_num_bins = config.rms_num_bins\n",
    "            self.rms_min = config.rms_min\n",
    "            self.mu_bins = get_mu_bins(self.rms_mu, self.rms_num_bins, self.rms_min)\n",
    "        # self.rgb_feature_dir = rgb_feature_dir\n",
    "        # self.flow_feature_dir = flow_feature_dir\n",
    "        # self.mel_dir = mel_dir\n",
    "        self.frame_dir = frame_dir\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        with open(list_file, encoding='utf-8') as f:\n",
    "            self.video_ids = [line.strip() for line in f]\n",
    "        self.video_class = os.path.basename(list_file).split(\"_\")[0]\n",
    "        self.video_transform = transforms.Compose(\n",
    "            self.generate_video_transform())\n",
    "\n",
    "    # def get_data_pair(self, video_id):\n",
    "    #     im_path = os.path.join(self.rgb_feature_dir, video_id+\".pkl\")\n",
    "    #     # flow_path = os.path.join(self.flow_feature_dir, video_id+\".pkl\")\n",
    "    #     # mel_path = os.path.join(self.mel_dir, video_id+\"_mel.npy\")\n",
    "    #     audio_path = os.path.join(self.mel_dir, video_id+\"_audio.npy\")\n",
    "    #     im = self.get_im(im_path)\n",
    "    #     # flow = self.get_flow(flow_path)\n",
    "    #     # mel = self.get_mel(mel_path)\n",
    "    #     rms = self.get_rms(audio_path)\n",
    "    #     if self.rms_discretize:\n",
    "    #         with torch.no_grad():\n",
    "    #             rms = discretize_rms(torch.tensor(rms.copy()), self.mu_bins)\n",
    "    #         rms = rms.long() # torch.tensor(rms.copy(), dtype=torch.long)\n",
    "    #     else:\n",
    "    #         rms = torch.tensor(rms.copy(), dtype=torch.float32)\n",
    "        \n",
    "    #     # feature = np.concatenate((im, flow), 1)\n",
    "    #     feature = im\n",
    "    #     feature = torch.FloatTensor(feature.astype(np.float32))\n",
    "    #     return (feature, rms, video_id, self.video_class)\n",
    "\n",
    "    # def get_mel(self, filename):\n",
    "    #     melspec = np.load(filename)\n",
    "    #     if melspec.shape[1] < self.mel_samples:\n",
    "    #         melspec_padded = np.zeros((melspec.shape[0], self.mel_samples))\n",
    "    #         melspec_padded[:, 0:melspec.shape[1]] = melspec\n",
    "    #     else:\n",
    "    #         melspec_padded = melspec[:, 0:self.mel_samples]\n",
    "    #     melspec_padded = torch.from_numpy(melspec_padded).float()\n",
    "    #     return melspec_padded\n",
    "    \n",
    "    def get_rgb_audio_pair(self, video_id):\n",
    "        # video_id = self.list_sample[index].split('_')[0]\n",
    "        frame_path = os.path.join(self.frame_dir, video_id)\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{video_id}.wav\")\n",
    "        \n",
    "        print(\"frame_path: \", frame_path)\n",
    "        print(\"audio_path: \", audio_path)\n",
    "        \n",
    "        frame_list = glob.glob(f'{frame_path}/img_*.jpg')\n",
    "        frame_list.sort()\n",
    "        imgs = self.read_image(frame_list)\n",
    "        \n",
    "        # audio_path = glob.glob(f\"{audio_path}/*.wav\")\n",
    "        # audio_path = audio_path[0]\n",
    "        audio, audio_sample_rate = sf.read(audio_path, start=0, stop=1000, dtype='float64', always_2d=True)\n",
    "        \n",
    "        frame_list = frame_list[0:int(self.video_samples)]\n",
    "        assert len(frame_list) == self.video_samples\n",
    "        \n",
    "        \n",
    "        # if imgs.shape[0] < self.video_samples:\n",
    "        #     imgs_padded = np.zeros((self.video_samples, imgs.shape[1]))\n",
    "        #     imgs_padded[0:imgs.shape[0], :] = imgs\n",
    "        # else:\n",
    "        #     imgs_padded = imgs[0:self.video_samples, :]\n",
    "        # assert imgs_padded.shape[0] == self.video_samples\n",
    "            \n",
    "        # stop training if frame_list lemngth is less than duration * frame_rate\n",
    "        # if len(frame_list) < duration * frame_rate:\n",
    "        #     raise RuntimeError(f\"frame_list length is less than duration * frame_rate: {len(frame_list)} < {duration * frame_rate}\")\n",
    "        \n",
    "        audio_len = int(self.duration * audio_sample_rate)\n",
    "        audio, audio_rate = sf.read(audio_path, start=0, stop=audio_len, dtype='float64', always_2d=True)\n",
    "        audio = audio.mean(-1)\n",
    "\n",
    "        onsets = librosa.onset.onset_detect(y=audio, sr=audio_rate, units='time', delta=0.3)\n",
    "        onsets = np.rint(onsets * self.frame_rate).astype(int)\n",
    "        # onsets[onsets>29] = 29\n",
    "        onsets[onsets > (self.video_samples - 1)] = self.video_samples - 1\n",
    "        label = torch.zeros(len(frame_list))\n",
    "        label[onsets] = 1\n",
    "\n",
    "        batch = {\n",
    "            'frames': imgs,\n",
    "            'label': label\n",
    "        }\n",
    "        return batch\n",
    "        \n",
    "    def read_image(self, frame_list):\n",
    "        imgs = []\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        for img_path in frame_list:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = convert_tensor(image)\n",
    "            imgs.append(image.unsqueeze(0))\n",
    "        # (T, C, H ,W)\n",
    "        print(len(imgs))\n",
    "        imgs = torch.cat(imgs, dim=0).squeeze()\n",
    "        imgs = self.video_transform(imgs)\n",
    "        imgs = imgs.permute(1, 0, 2, 3)\n",
    "        # (C, T, H ,W)\n",
    "        return imgs\n",
    "    \n",
    "    def generate_video_transform(self):\n",
    "        resize_funct = transforms.Resize((128, 128))\n",
    "        if self.split == 'train':\n",
    "            crop_funct = transforms.RandomCrop(\n",
    "                (112, 112))\n",
    "            color_funct = transforms.ColorJitter(\n",
    "                brightness=0.1, contrast=0.1, saturation=0, hue=0)\n",
    "        else:\n",
    "            crop_funct = transforms.CenterCrop(\n",
    "                (112, 112))\n",
    "            color_funct = transforms.Lambda(lambda img: img)\n",
    "\n",
    "        vision_transform_list = [\n",
    "            resize_funct,\n",
    "            crop_funct,\n",
    "            color_funct,\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "        return vision_transform_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return self.get_data_pair(self.video_ids[index])\n",
    "        return self.get_rgb_audio_pair(self.video_ids[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceramic_train:  /mnt/GreatestHits/filelists/ceramic_train.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2015-03-31-01-20-00_0',\n",
       " '2015-03-30-01-40-09_1',\n",
       " '2015-03-25-00-36-19_0',\n",
       " '2015-03-31-00-58-20_0',\n",
       " '2015-03-20-01-25-43_0',\n",
       " '2015-03-20-01-25-43_1',\n",
       " '2015-02-22-15-02-49_0',\n",
       " '2015-02-22-15-02-49_1',\n",
       " '2015-02-22-15-02-49_2',\n",
       " '2015-02-22-15-02-49_3',\n",
       " '2015-03-27-23-27-01_1',\n",
       " '2015-03-20-01-55-25_0',\n",
       " '2015-03-20-01-55-25_1',\n",
       " '2015-03-29-17-16-02_0',\n",
       " '2015-03-29-01-18-39_4',\n",
       " '2015-03-29-17-17-14_0',\n",
       " '2015-02-16-16-49-06_0',\n",
       " '2015-02-16-16-49-06_5',\n",
       " '2015-02-16-16-49-06_6',\n",
       " '2015-03-20-01-22-22_0',\n",
       " '2015-03-20-01-22-22_1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceramic_train = config.data.training_files[2]\n",
    "print(\"ceramic_train: \", ceramic_train)\n",
    "with open(ceramic_train, encoding='utf-8') as f:\n",
    "    test_video_ids = [line.strip() for line in f]\n",
    "test_video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_path:  /mnt/GreatestHits/features/None/OF_10s_15fps/2015-02-21-17-48-19_0\n",
      "audio_path:  /mnt/GreatestHits/features/None/audio_10s_16000hz/2015-02-21-17-48-19_0.wav\n",
      "151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dabin/anaconda3/envs/condfoley/lib/python3.8/site-packages/librosa/filters.py:238: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_path:  /mnt/GreatestHits/features/carpet/OF_10s_15fps/2015-03-30-01-01-15_1\n",
      "audio_path:  /mnt/GreatestHits/features/carpet/audio_10s_16000hz/2015-03-30-01-01-15_1.wav\n",
      "151\n",
      "frame_path:  /mnt/GreatestHits/features/ceramic/OF_10s_15fps/2015-03-25-00-36-19_0\n",
      "audio_path:  /mnt/GreatestHits/features/ceramic/audio_10s_16000hz/2015-03-25-00-36-19_0.wav\n",
      "151\n",
      "frame_path:  /mnt/GreatestHits/features/cloth/OF_10s_15fps/2015-02-23-20-09-50_2\n",
      "audio_path:  /mnt/GreatestHits/features/cloth/audio_10s_16000hz/2015-02-23-20-09-50_2.wav\n",
      "151\n",
      "frame_path:  /mnt/GreatestHits/features/dirt/OF_10s_15fps/2015-09-29-15-44-54-933_0\n",
      "audio_path:  /mnt/GreatestHits/features/dirt/audio_10s_16000hz/2015-09-29-15-44-54-933_0.wav\n",
      "151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-43bfe77f1bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1ea95cca5bd7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# return self.get_data_pair(self.video_ids[index])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rgb_audio_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1ea95cca5bd7>\u001b[0m in \u001b[0;36mget_rgb_audio_pair\u001b[0;34m(self, video_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{frame_path}/img_*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mframe_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# audio_path = glob.glob(f\"{audio_path}/*.wav\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1ea95cca5bd7>\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(self, frame_list)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# (C, T, H ,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0malign_corners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bicubic\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bicubic\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condfoley/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4053\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0malign_corners\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4055\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upsample_bilinear2d_aa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4056\u001b[0m         \u001b[0;31m# Two levels are necessary to prevent TorchScript from touching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4057\u001b[0m         \u001b[0;31m# are_deterministic_algorithms_enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "for dirs in zip(config.data.training_files, config.data.frame_dirs, config.data.audio_dirs):\n",
    "    trainset = VideoAudioDataset(*dirs, config.data)\n",
    "    trainset[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condfoley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
