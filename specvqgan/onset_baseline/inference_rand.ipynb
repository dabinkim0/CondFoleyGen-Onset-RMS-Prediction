{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import utils, torch_utils\n",
    "\n",
    "from config import init_args\n",
    "import data\n",
    "from data.config import _C as config\n",
    "\n",
    "import models\n",
    "from models import *\n",
    "import librosa\n",
    "import copy\n",
    "import json\n",
    "from moviepy.editor import VideoFileClip\n",
    "import argparse\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window:  0.05\n",
      "window:  1\n"
     ]
    }
   ],
   "source": [
    "FRAME_RATE = 30\n",
    "window = 0.05\n",
    "print(\"window: \", window)\n",
    "window = librosa.time_to_samples(window, sr=FRAME_RATE)\n",
    "print(\"window: \", window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "# 16000\n",
    "FRAME_RATE = 30\n",
    "DURATION = 10\n",
    "\n",
    "def onset_nms(onset, confidence, window=0.05):\n",
    "    window = librosa.time_to_samples(window, sr=FRAME_RATE)\n",
    "    if window < 1:\n",
    "        return onset\n",
    "    \n",
    "    # Non-Maximum Suppression, NMS\n",
    "    # : descard the duplicated onset\n",
    "    onset_remain = onset.tolist()\n",
    "    output = []\n",
    "    # descending order of confidence of predicted onset\n",
    "    sorted_idx = np.argsort(confidence)[::-1]   \n",
    "    \n",
    "    for idx in sorted_idx:\n",
    "        cur = onset[idx]\n",
    "        if cur not in onset_remain:\n",
    "            continue\n",
    "        output.append(cur)\n",
    "        onset_remain.remove(cur)\n",
    "        \n",
    "        for o in onset_remain:\n",
    "            # if abs(cur - o) < window:\n",
    "            #     onset_remain.remove(o)\n",
    "            if abs(cur - o) <= window:\n",
    "                onset_remain.remove(o)\n",
    "                \n",
    "    return np.array(sorted(output))\n",
    "\n",
    "\n",
    "def eval_onsets(onset_gt, onset_pred, logits, delta=0.1, conf_interval=0.05, keys=None): \n",
    "    # delta 0.05 conf_interval 0.05, delta 0.05 conf_interval 0.025\n",
    "    # conf_interval: 0.05sec (0 frame), 0.1sec (1 frame)\n",
    "    \n",
    "    conf_interval = librosa.time_to_samples(conf_interval, sr=FRAME_RATE)        \n",
    "    logits = np.abs(logits)                                                     \n",
    "    logits = (logits - np.min(logits)) / (np.max(logits) - np.min(logits))\n",
    "    \n",
    "    if len(logits) < DURATION * FRAME_RATE:\n",
    "        print(\"logits len: \", len(logits))\n",
    "        print(\"logits is too short\")\n",
    "        \n",
    "    # onset_pred : frame numbers of predicted onsets\n",
    "    # confidence : the highest probability in the predicred onset frame\n",
    "    # print(\"Confidence interval: \", conf_interval)\n",
    "    \n",
    "    if conf_interval > 0:\n",
    "        confidence = [np.max(logits[max(0, o-conf_interval):min(len(logits)-1, o+conf_interval)]) for o in onset_pred if len(logits) > 0]\n",
    "    else:\n",
    "        confidence = [np.max(logits[o]) for o in onset_pred if len(logits) > 0]\n",
    "    \n",
    "    # OnsetNMS\n",
    "    onset_pred = onset_nms(onset_pred, confidence)\n",
    "    onset_pred_keep = onset_pred\n",
    "    onset_pred_onuse = copy.deepcopy(onset_pred.tolist())       \n",
    "    onset_pred_res = [0 for _ in onset_pred_onuse]\n",
    "    \n",
    "    hit_cnt = 0\n",
    "    y_gt = []\n",
    "    y_pred = []\n",
    "    \n",
    "    delta = librosa.time_to_samples(delta, sr=FRAME_RATE)   # 0.1 sec, 1 frame\n",
    "\n",
    "    for o in onset_gt:\n",
    "        # frame level difference between onset_gt and onset_pred\n",
    "        diff = [abs(o2 - o) for o2 in onset_pred_onuse]\n",
    "        # indices of onset_pred in the window\n",
    "        idx_in_window = [idx for idx in range(len(onset_pred_onuse)) if diff[idx] < delta]  \n",
    "        \n",
    "        if len(idx_in_window) == 0:\n",
    "            y_gt.append(1)\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            # confidence of predicted onset in the window\n",
    "            conf_in_window = [logits[onset_pred_onuse[idx]] for idx in idx_in_window]  \n",
    "             # the highest confidence index in the window \n",
    "            max_conf_idx = np.argsort(conf_in_window)[-1]                              \n",
    "            match_idx = idx_in_window[max_conf_idx]                                     \n",
    "            hit_cnt += 1\n",
    "            y_gt.append(1)\n",
    "            \n",
    "            if conf_interval > 0:\n",
    "                conf = np.max(logits[max(0, onset_pred_onuse[match_idx]-conf_interval):min(len(logits)-1, onset_pred_onuse[match_idx]+conf_interval)])\n",
    "            else:\n",
    "                conf = np.max(logits[onset_pred_onuse[match_idx]])\n",
    "                \n",
    "            for i in range(len(onset_pred_keep)):\n",
    "                if onset_pred_keep[i] == onset_pred_onuse[match_idx]:\n",
    "                    onset_pred_res[i] = 1\n",
    "            y_pred.append(conf)\n",
    "            onset_pred_onuse.remove(onset_pred_onuse[match_idx])     \n",
    "            if len(onset_pred_onuse) == 0:\n",
    "                break\n",
    "\n",
    "    for o in onset_pred_onuse:\n",
    "        y_gt.append(0)\n",
    "        if conf_interval > 0:\n",
    "            y_pred.append(np.max(logits[max(0, o-conf_interval):min(len(logits)-1, o+conf_interval)]))\n",
    "        else:\n",
    "            y_pred.append(np.max(logits[o]))\n",
    "            \n",
    "    y_pred_binarized = [1 if y >= 0.5 else 0 for y in y_pred]\n",
    "    \n",
    "    # ACC calculation A\n",
    "    # TP = sum([1 for i in range(len(y_gt)) if y_gt[i] == 1 and y_pred_binarized[i] == 1])\n",
    "    # TN = sum([1 for i in range(len(y_gt)) if y_gt[i] == 0 and y_pred_binarized[i] == 0])\n",
    "    # FP = sum([1 for i in range(len(y_gt)) if y_gt[i] == 0 and y_pred_binarized[i] == 1])\n",
    "    # FN = sum([1 for i in range(len(y_gt)) if y_gt[i] == 1 and y_pred_binarized[i] == 0])\n",
    "    # acc = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) != 0 else 0\n",
    "    \n",
    "    # ACC calculation B\n",
    "    acc = accuracy_score(y_gt, y_pred_binarized)\n",
    "    ap = average_precision_score(y_gt, y_pred)\n",
    "    pr, rc, th = precision_recall_curve(y_gt, y_pred)\n",
    "    \n",
    "    if keys != None and args.plt:\n",
    "        plt.plot(rc, pr)\n",
    "        plt.ylim((0, 1))\n",
    "        plt.savefig(f'tmp/pc_rc_curve_tar_{keys[0]}_cond_{keys[1]}.jpg')\n",
    "        plt.close()\n",
    "\n",
    "    return acc, ap, onset_pred_res, y_gt, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. done\n",
      "4. done\n",
      "Number of parameters in the model: 31365918\n",
      "=> loading checkpoint '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec_rand/checkpoint_ep100.pth.tar'\n",
      "=> loaded checkpoint '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec_rand/checkpoint_ep100.pth.tar' (epoch 100)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Model checkpoint 2sec / 100epochs\n",
    "# checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_2sec/checkpoint_ep100.pth.tar'\n",
    "# Model checkpoint 10sec / 100epochs\n",
    "# checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec/checkpoint_ep100.pth.tar'\n",
    "# Model checkpoint random 10sec / 100epochs, weight decay & batch size adjusted\n",
    "checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec_rand/checkpoint_ep100.pth.tar'\n",
    "\n",
    "args = init_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_rate = SAMPLE_RATE\n",
    "frame_rate = FRAME_RATE\n",
    "duration = DURATION\n",
    "\n",
    "# test_dataset = data.VideoAudioDataset(args, split='test')\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     num_workers=args.num_workers,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=False)\n",
    "\n",
    "print(\"3. done\")\n",
    "\n",
    "test_dataset = None\n",
    "for dirs in zip(config.data.test_files, config.data.frame_dirs, config.data.audio_dirs):\n",
    "    if test_dataset is None:\n",
    "        test_dataset = data.VideoAudioDataset(*dirs, config.data, split='test')\n",
    "    else:\n",
    "        test_dataset += data.VideoAudioDataset(*dirs, config.data, split='test')\n",
    "        \n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        num_workers=args.num_workers, \n",
    "                        shuffle=False,\n",
    "                        batch_size=1, \n",
    "                        pin_memory=True, \n",
    "                        drop_last=False)  \n",
    "\n",
    "print(\"4. done\")\n",
    "\n",
    "net = models.VideoOnsetNet(pretrained=False).to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params}\")\n",
    "\n",
    "# load the trained model weights\n",
    "net, _ = torch_utils.load_model(checkpoint_path, net, device=device, strict=True)\n",
    "net.eval()\n",
    "\n",
    "all_y_gt = []\n",
    "all_y_pred = []\n",
    "all_acc = []\n",
    "\n",
    "for step, batch in enumerate(test_loader):\n",
    "    inputs = {'frames': batch['frames'].to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = net(inputs)\n",
    "    \n",
    "    # ground truth onsets\n",
    "    target = batch['label'].to(device)\n",
    "    target_np = target.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy() \n",
    "    \n",
    "    # sigmoid to pred_np\n",
    "    logits = pred_np[0]                                      # min -inf, max inf\n",
    "    pred_prob = 1 / (1 + np.exp(-pred_np))                   # sigmoid : min 0, max 1\n",
    "    pred_np = np.where(pred_prob > 0.5, 1.0, 0.0)            # binary : min 0, max 1\n",
    "\n",
    "    target_frames = np.where(target_np[0] == 1)[0] \n",
    "    pred_frames = np.where(pred_np[0] == 1)[0]\n",
    "    \n",
    "    onset_gt = target_frames            # gt (min 0, max 149, len: num of gt onsets) (15fps, 10sec)\n",
    "    onset_pred = pred_frames            # pred (min 0, max 149, len: num of pred onsets) (15fps, 10sec)\n",
    "    \n",
    "    acc, ap, onset_pred_res, y_gt, y_pred = eval_onsets(onset_gt, onset_pred, logits) \n",
    "    # print(f\"Test Set {step} - Accuracy: {acc:.2f}, AP: {ap:.2f}\")\n",
    "    \n",
    "    all_y_gt.extend(y_gt)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    all_acc.append(acc)\n",
    "    \n",
    "overall_acc = np.mean(all_acc)\n",
    "overall_ap = average_precision_score(all_y_gt, all_y_pred)\n",
    "pr, rc, th = precision_recall_curve(all_y_gt, all_y_pred)\n",
    "pr_auc = auc(rc, pr)\n",
    "\n",
    "print(f\"Overall Test Set - Accuracy: {overall_acc:.2f}, AP: {overall_ap:.2f}\")\n",
    "\n",
    "plt.plot(rc, pr)\n",
    "plt.title(f'Precision-Recall Curve\\nAP: {overall_ap:.2f}, PR-AUC: {pr_auc:.2f}')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# A. 2 sec, without Confidence Interval and Onset NMS\n",
    "# A-1. accuracy_score in sklearn.metrics \n",
    "# Overall Test Set - Accuracy: 0.50, AP: 0.70\n",
    "\n",
    "# A. 10 sec, without Confidence Interval and Onset NMS\n",
    "# A-1. accuracy_score in sklearn.metrics \n",
    "# Number of parameters in the model: 31365918\n",
    "# Overall Test Set - Accuracy: 0.47, AP: 0.72, PR-AUC: 0.70\n",
    "\n",
    "# SyncFusion\n",
    "# Accuracy: 0.49.39, AP: 0.88.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint 2sec / 100epochs\n",
    "# checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_2sec/checkpoint_ep100.pth.tar'\n",
    "# Model checkpoint 10sec / 100epochs\n",
    "# checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec/checkpoint_ep100.pth.tar'\n",
    "checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_10sec_rand/checkpoint_ep100.pth.tar'\n",
    "\n",
    "# Model checkpoint 2sec / 100epochs, weight decay & batch size adjusted\n",
    "# checkpoint_path = '/home/dabin/video2foley/CondFoleyGen/specvqgan/onset_baseline/checkpoints/EXP_2sec/checkpoint_ep100.pth.tar'\n",
    "\n",
    "args = init_args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_dataset = data.GreatestHitDataset(args, split='test')\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,   # args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False)\n",
    "\n",
    "net = models.VideoOnsetNet(pretrained=False).to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "net, _ = torch_utils.load_model(checkpoint_path, net, device=device, strict=True)\n",
    "net.eval()\n",
    "\n",
    "all_y_gt = []\n",
    "all_y_pred = []\n",
    "all_acc = []\n",
    "\n",
    "for step, batch in enumerate(test_loader):\n",
    "    inputs = {\n",
    "        'frames': batch['frames'].to(device)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = net(inputs)\n",
    "    \n",
    "    # Ground truth onsets\n",
    "    target = batch['label'].to(device)\n",
    "    target_np = target.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    \n",
    "    logits = pred_np[0]\n",
    "    \n",
    "    # Sigmoid to pred_np\n",
    "    pred_prob = 1 / (1 + np.exp(-pred_np))\n",
    "    pred_np = np.where(pred_prob > 0.5, 1.0, 0.0)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 4)) \n",
    "    \n",
    "    # Predicted onsets (Sigmoid)\n",
    "    plt.bar(range(len(pred_prob[0])), pred_prob[0], alpha=0.3, label='Predicted Onsets Probability (Sigmoid)')\n",
    "    \n",
    "    # Predicted onsets\n",
    "    pred_np = np.where(pred_np[0] == 1)[0]\n",
    "    plt.vlines(pred_np, 0.0, 0.5, colors='b', linestyles='solid', label='Predicted Onsets')\n",
    "\n",
    "    # Ground truth onsets\n",
    "    target_np = np.where(target_np[0] == 1)[0]\n",
    "    plt.vlines(target_np, 0.5, 1.0, colors='r', linestyles='solid', label='Ground Truth Onsets')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks([0.25, 0.75], ['Pred', 'GT'])\n",
    "    plt.xlabel('Frame (15fps, 2sec)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'Test Sample {step+1}')\n",
    "    plt.show()\n",
    "    \n",
    "    if step >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dabin/video2sound/CondFoleyGen/data/greatesthit_test_2.00.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf7f5e8b0ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlist_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/dabin/video2sound/CondFoleyGen/data/greatesthit_test_2.00.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlist_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sample\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/dabin/video2sound/CondFoleyGen/data/greatesthit_test_2.00.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "index = 0\n",
    "\n",
    "list_sample = '/home/dabin/video2sound/CondFoleyGen/data/greatesthit_test_2.00.json'\n",
    "with open(list_sample, \"r\") as f:\n",
    "    list_sample = json.load(f)\n",
    "if args.max_sample > 0:\n",
    "    list_sample = list_sample[:args.max_sample]\n",
    "\n",
    "info = list_sample[index].split('_')[0]\n",
    "video_path = os.path.join('/home/dabin/video2sound/CondFoleyGen/data', 'greatesthit', 'greatesthit-process-resized', info)\n",
    "frame_path = os.path.join(video_path, 'frames')\n",
    "audio_path = os.path.join(video_path, 'audio')\n",
    "audio_path = glob.glob(f\"{audio_path}/*.wav\")\n",
    "audio_path = audio_path[0]\n",
    "print(\"Audio path: \", audio_path)\n",
    "\n",
    "audio, sr = sf.read(audio_path, start=0, stop=1000, dtype='float64', always_2d=True)\n",
    "\n",
    "# Listen the sample audio in jupyter notebook\n",
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condfoley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
